{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **SVM & Naive Bayes | Assignment**\n"
      ],
      "metadata": {
        "id": "h0njrRfpbPD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1**: What is a Support Vector Machine (SVM), and how does it work?"
      ],
      "metadata": {
        "id": "aUd8liwpbUYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A **Support Vector Machine (SVM)** is a supervised machine learning algorithm used for **classification** and **regression** tasks. Its main goal is to find the optimal decision boundary (called a *hyperplane*) that separates different classes in the feature space.\n",
        "\n",
        "####How SVM Works:\n",
        "\n",
        "1. **Hyperplane**:\n",
        "   SVM finds the hyperplane that best separates the data into classes. In 2D, this is just a line; in higher dimensions, it becomes a plane or hyperplane.\n",
        "\n",
        "2. **Maximum Margin**:\n",
        "   SVM chooses the hyperplane that has the **maximum margin** — the distance between the hyperplane and the nearest data points from each class. These nearest points are called **support vectors**.\n",
        "\n",
        "3. **Support Vectors**:\n",
        "   These are the critical elements of the training set. They lie closest to the decision boundary and influence its position.\n",
        "\n",
        "4. **Linear vs. Non-Linear SVM**:\n",
        "\n",
        "   * If data is **linearly separable**, a straight hyperplane is enough.\n",
        "   * For **non-linearly separable** data, SVM uses a **kernel trick** to map data into a higher-dimensional space where a linear separator can be found.\n",
        "\n",
        "5. **Kernel Trick**:\n",
        "   A mathematical technique that transforms the input data into a higher dimension to make it possible to find a separating hyperplane. Common kernels include:\n",
        "\n",
        "   * Linear\n",
        "   * Polynomial\n",
        "   * Radial Basis Function (RBF)\n",
        "\n",
        "6. **Soft Margin**:\n",
        "   SVM allows some misclassifications for better generalization. The **C parameter** controls the trade-off between a wide margin and classification accuracy.\n"
      ],
      "metadata": {
        "id": "SXsWeS8Nbz5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2**: Explain the difference between Hard Margin and Soft Margin SVM.\n"
      ],
      "metadata": {
        "id": "YiUg3X6qcZUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Difference Between Hard Margin and Soft Margin SVM:\n",
        "\n",
        "| Feature                          | **Hard Margin SVM**                                     | **Soft Margin SVM**                                                               |\n",
        "| -------------------------------- | ------------------------------------------------------- | --------------------------------------------------------------------------------- |\n",
        "| **Definition**                   | Assumes data is perfectly separable with no errors.     | Allows some misclassifications or margin violations.                              |\n",
        "| **Tolerance to Errors**          | No tolerance for misclassified points.                  | Allows some points to be within the margin or misclassified.                      |\n",
        "| **Use Case**                     | Suitable when data is linearly separable without noise. | Suitable for noisy or overlapping data.                                           |\n",
        "| **Generalization**               | May **overfit** if applied to real-world noisy data.    | Better **generalization** on unseen data.                                         |\n",
        "| **Margin Type**                  | Rigid and strict margin – no data points inside it.     | Flexible margin controlled by a regularization parameter.                         |\n",
        "| **Regularization Parameter (C)** | Not used.                                               | Uses **C** to control the trade-off between margin size and classification error. |\n",
        "| **Robustness**                   | Less robust to outliers.                                | More robust to outliers due to flexibility.                                       |\n",
        "\n"
      ],
      "metadata": {
        "id": "TkSnkj9DcNZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3**: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n"
      ],
      "metadata": {
        "id": "A3o5wijuc1F-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is the Kernel Trick in SVM?\n",
        "\n",
        "The **Kernel Trick** is a mathematical technique used in SVM to handle **non-linearly separable data**. Instead of explicitly transforming data into a higher-dimensional space, the kernel function computes the **inner product** of two data points in that space **without actually performing the transformation**. This makes computations efficient and allows SVM to find a separating hyperplane in a more complex space.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Use the Kernel Trick?\n",
        "\n",
        "* Real-world data is often **not linearly separable**.\n",
        "* The kernel trick helps **map** data to a higher dimension where a linear separator **can** be found.\n",
        "* It avoids the **computational cost** of actual transformation.\n",
        "\n",
        "\n",
        "\n",
        "### Use Case of RBF Kernel:\n",
        "\n",
        "* Best suited for **non-linear classification problems** where the decision boundary is **not a straight line**.\n",
        "* Example: Classifying images of animals where features like size, shape, and texture do not follow a linear pattern.\n",
        "* The RBF kernel can create **curved decision boundaries** to capture complex patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "SaT2_FwsdCUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4**: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n"
      ],
      "metadata": {
        "id": "xGSkaB3tdJKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "A **Naïve Bayes Classifier** is a **probabilistic machine learning algorithm** based on **Bayes’ Theorem**. It is mainly used for **classification tasks**, such as text classification, spam detection, and sentiment analysis.\n",
        "\n",
        "It calculates the **probability of a class given a set of features** and predicts the class with the highest probability.\n",
        "\n",
        "\n",
        "\n",
        "#### Bayes’ Theorem Recap:\n",
        "\n",
        "$$\n",
        "P(C \\mid X) = \\frac{P(X \\mid C) \\cdot P(C)}{P(X)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $P(C \\mid X)$ = Probability of class **C** given features **X**\n",
        "* $P(X \\mid C)$ = Likelihood of features **X** given class **C**\n",
        "* $P(C)$ = Prior probability of class **C**\n",
        "* $P(X)$ = Probability of features **X**\n",
        "\n",
        "\n",
        "\n",
        "#### Example Use Cases:\n",
        "\n",
        "* Email spam filtering\n",
        "* Sentiment analysis of reviews\n",
        "* Document classification\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WCrd-F_Odj2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5**: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?"
      ],
      "metadata": {
        "id": "CpD8Hrjudka6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **1. Gaussian Naïve Bayes**\n",
        "\n",
        "* **Used for**: **Continuous (real-valued)** features\n",
        "* **Assumption**: Features follow a **normal (Gaussian) distribution**\n",
        "* **How it works**: Calculates the likelihood of features using the **probability density function** of the normal distribution\n",
        "* **Use Case Examples**:\n",
        "\n",
        "  * Iris flower classification (based on petal/sepal measurements)\n",
        "  * Medical data with continuous measurements (e.g., blood pressure, age, weight)\n",
        "\n",
        "\n",
        "\n",
        "#### **2. Multinomial Naïve Bayes**\n",
        "\n",
        "* **Used for**: **Discrete count** features (typically **word counts** in text)\n",
        "* **Assumption**: Features represent the **number of times** an event (like a word) occurs\n",
        "* **How it works**: Computes probabilities based on **term frequencies** in each class\n",
        "* **Use Case Examples**:\n",
        "\n",
        "  * Text classification (e.g., spam detection, news categorization)\n",
        "  * Document classification where features are word frequencies\n",
        "\n",
        "\n",
        "\n",
        "#### **3. Bernoulli Naïve Bayes**\n",
        "\n",
        "* **Used for**: **Binary (0/1)** features (whether a word is **present or absent**)\n",
        "* **Assumption**: Features are boolean-valued\n",
        "* **How it works**: Evaluates features as **present (1)** or **absent (0)** in each class\n",
        "* **Use Case Examples**:\n",
        "\n",
        "  * Binary text classification with word presence/absence (e.g., spam detection using bag-of-words with binary features)\n",
        "  * Any classification where input features are binary (e.g., medical diagnosis with symptom present/absent)\n",
        "\n",
        "\n",
        "### Choosing the Right Variant:\n",
        "\n",
        "* Use **Gaussian** for real-valued inputs.\n",
        "* Use **Multinomial** when working with count data like term frequency.\n",
        "* Use **Bernoulli** for binary or boolean input features.\n"
      ],
      "metadata": {
        "id": "yOFfdVnFd2DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Info:\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "\n",
        "**Question 6**: Write a Python program to:\n",
        "\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear\n",
        "\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "XbitYtVxeQLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train an SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of the SVM model:\", accuracy)\n",
        "print(\"Support vectors:\\n\", svm_model.support_vectors_)\n",
        "print(\"Number of support vectors for each class:\", svm_model.n_support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ty8k-a-e4vT",
        "outputId": "7cccd256-3b0c-4cad-9321-f6fa7935457e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM model: 1.0\n",
            "Support vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "Number of support vectors for each class: [ 3 11 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 7**: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "FMJWh9k_fJI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__jCKnCdfg8N",
        "outputId": "ee275613-1a10-4ea4-bef2-af87d31a589b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 8**: Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Dm0OrJqIfqm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']  # Using RBF kernel\n",
        "}\n",
        "\n",
        "# Create and fit the GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbUHgcTsf8C5",
        "outputId": "f6b7db52-22eb-461b-fc9c-a462ca4afb32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on test set: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 9**: Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "3snGoSAYgBQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a binary classification subset (e.g., 'sci.space' vs 'rec.sport.hockey')\n",
        "categories = ['sci.space', 'rec.sport.hockey']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = nb_model.predict_proba(X_test_vec)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Calculate and print ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXEwPelGgTtE",
        "outputId": "8c2dc1a0-c143-4d46-ccbe-cb8e1eeaba83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9931531531531532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 10**: Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "p1cO15CDgdRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER:**\n",
        "####  1. **Preprocessing the Data**\n",
        "\n",
        "* **Handle missing data**: Remove or fill null values in email text.\n",
        "* **Text cleaning**: Lowercase, remove punctuation, stopwords (optional).\n",
        "* **Vectorization**: Use `TfidfVectorizer` to handle diverse vocabulary and reduce the impact of common words.\n",
        "\n",
        "\n",
        "\n",
        "####  2. **Model Choice**\n",
        "\n",
        "* **Naïve Bayes** is preferred for **text classification** due to:\n",
        "\n",
        "  * Simplicity\n",
        "  * Speed\n",
        "  * Strong performance on high-dimensional, sparse data (like emails)\n",
        "* **SVM** is powerful but slower and less interpretable for large datasets.\n",
        "\n",
        "\n",
        "\n",
        "####  3. **Handling Class Imbalance**\n",
        "\n",
        "* Use **class weighting** (in SVM) or **resampling techniques** (like SMOTE).\n",
        "* In Naïve Bayes, class imbalance can be handled by adjusting thresholds or using balanced datasets.\n",
        "\n",
        "\n",
        "\n",
        "####  4. **Evaluation Metrics**\n",
        "\n",
        "Use metrics that reflect performance on **imbalanced data**:\n",
        "\n",
        "* **Precision**\n",
        "* **Recall**\n",
        "* **F1-score**\n",
        "* **ROC-AUC Score**\n",
        "\n",
        "\n",
        "\n",
        "####  5. **Business Impact**\n",
        "\n",
        "* Automating spam detection reduces time and manual filtering\n",
        "* Improves employee productivity\n",
        "* Protects against phishing/malware\n",
        "* Enhances email infrastructure efficiency\n",
        "\n"
      ],
      "metadata": {
        "id": "22dWnTh4hnXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulate: Use 'talk.politics.misc' as 'spam' and 'sci.med' as 'not spam'\n",
        "categories = ['talk.politics.misc', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Convert to DataFrame for preprocessing\n",
        "df = pd.DataFrame({'text': data.data, 'target': data.target})\n",
        "\n",
        "# Introduce some missing values (simulating incomplete data)\n",
        "np.random.seed(42)\n",
        "missing_indices = np.random.choice(df.index, size=10, replace=False)\n",
        "df.loc[missing_indices, 'text'] = None\n",
        "\n",
        "# Handle missing data\n",
        "df.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Create pipeline: TF-IDF + Naive Bayes\n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(),\n",
        "    MultinomialNB()\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = pipeline.predict(X_test)\n",
        "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=categories))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRTgWrpAg4fq",
        "outputId": "afee0113-d211-45a9-fa0d-2cc441a8c7bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "talk.politics.misc       0.89      0.98      0.93       302\n",
            "           sci.med       0.97      0.84      0.90       225\n",
            "\n",
            "          accuracy                           0.92       527\n",
            "         macro avg       0.93      0.91      0.92       527\n",
            "      weighted avg       0.92      0.92      0.92       527\n",
            "\n",
            "ROC-AUC Score: 0.9854598969830758\n"
          ]
        }
      ]
    }
  ]
}